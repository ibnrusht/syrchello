{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9263bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     127219\n",
       "type                   127219\n",
       "date                   127219\n",
       "date_unixtime          127219\n",
       "actor                     279\n",
       "actor_id                  291\n",
       "action                    291\n",
       "title                      22\n",
       "text                   127219\n",
       "photo                    2037\n",
       "width                    5055\n",
       "height                   5055\n",
       "from                   126841\n",
       "from_id                126928\n",
       "forwarded_from            707\n",
       "saved_from                118\n",
       "reply_to_message_id     77729\n",
       "edited                   5790\n",
       "edited_unixtime          5790\n",
       "file                     3154\n",
       "thumbnail                2900\n",
       "media_type               3083\n",
       "sticker_emoji            2544\n",
       "mime_type                 582\n",
       "duration_seconds          538\n",
       "poll                        7\n",
       "members                   271\n",
       "via_bot                   179\n",
       "message_id                 17\n",
       "performer                  21\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "\n",
    "with open('result.json') as res:\n",
    "    f = res.read()\n",
    "    jf = json.loads(f)\n",
    "    df = pd.DataFrame(jf['messages'])\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "984e829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "syr = df.loc[df['from'] == '***', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b53cfc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0, 250)\n",
    "df['text'].apply(len).hist(bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78700122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='text', ylabel='Count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fid, ax = plt.subplots()\n",
    "ax.set_xlim(0, 250)\n",
    "mask = df['text'].apply(len) > 5\n",
    "print(df['text'].apply(len).max())\n",
    "sns.histplot(df.loc[mask, 'text'].apply(len), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18f2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask2 = df['text'].apply(len) == df['text'].apply(len).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a308987b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127219, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762ebca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122328, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df[mask2].index, axis=0, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21533049",
   "metadata": {},
   "outputs": [],
   "source": [
    "func1 = lambda x: re.sub(\"\\n\", \" \", x) \n",
    "func = lambda x: re.sub(\"[^абвгдеёжзийклмнопрстуфхцчшщэьыюяъ ,.!?]\", \"\", x)\n",
    "func2 = lambda x: re.sub(\"аа\", \" \", x)\n",
    "func4 = lambda x: re.sub(\" \\,\", \"\", x)\n",
    "func5 = lambda x: re.sub(\" +\", \" \", x)\n",
    "func6 = lambda x: re.sub(\" \\.\", \"\", x)\n",
    "func7 = lambda x: re.sub(\"\\.\\.+\", \"\", x)\n",
    "text = df['text'].apply(str).str.lower().apply(func1).apply(func).apply(func2)\n",
    "syr_text = syr.apply(str).str.lower().apply(func7).apply(func1).apply(func).apply(func4).apply(func6).apply(func2).apply(func5)\n",
    "syr_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c86789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = text.str.cat(sep=' ')\n",
    "text = text.apply(func4)\n",
    "# text = text.apply(func5)\n",
    "# text = text.apply(func6)\n",
    "mask = [True if '.' == item else False for item in text]\n",
    "syr_mask = [True if '.' == item else False for item in syr_text]\n",
    "text[mask] = '0'\n",
    "syr[syr_mask] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7d5610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = [True if ' ' == item else False for item in text]\n",
    "syr_mask = [True if ' ' == item else False for item in syr_text]\n",
    "text[mask] = '0'\n",
    "syr_text[syr_mask] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5e51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = [True if '' == item else False for item in text]\n",
    "syr_mask = [True if '' == item else False for item in syr_text]\n",
    "text[mask] = '0'\n",
    "syr_text[syr_mask] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4260b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122328\n",
      "6250\n",
      "120600\n",
      "5750\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(syr_text))\n",
    "text = text.loc[~(text == '0')]\n",
    "syr_text = syr_text.loc[~(syr_text == '0')]\n",
    "print(len(text))\n",
    "print(len(syr_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43789314",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.drop_duplicates(keep=False, inplace=True)\n",
    "syr_text.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1193acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_text = text.loc[text.apply(len) >= 16].str.cat(sep=' ')\n",
    "text = text.str.cat(sep=' ')\n",
    "syr_text = syr_text.str.cat(sep=' ')\n",
    "len(syr_text)\n",
    "with open('text.txt', 'w') as f:\n",
    "    f.write(text)\n",
    "\n",
    "with open('syr_text.txt', 'w') as s:\n",
    "    s.write(syr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96a542f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197945"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(syr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "773af05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('iliada.txt') as text_file:\n",
    "    text_sample = text_file.readlines()\n",
    "text_sample = ' '.join(text_sample)\n",
    "text_sample = re.sub(\"[^абвгдеёжзийклмнопрстуфхцчшщэьыюяъ ,.!?\\-ЙЦУКЕНГШЩЗХЪФЫВАПРОЛДЖЭЯЧСМИТЬБЮЁ]\", \"\", text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b8935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80283f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'о', 'а', 'е', 'т', 'и', 'н', 'с', 'р', 'в', 'к', 'л', 'м', 'п', 'у', 'д', 'ь', ',', 'ы', 'б', 'я', 'г', 'ч', 'з', 'х', 'ж', 'й', '.', 'ш', 'э', '?', 'ю', 'ц', 'ё', 'щ', 'ф', '!', 'ъ']\n"
     ]
    }
   ],
   "source": [
    "def text_to_seq(text_sample):\n",
    "    char_count = Counter(text_sample)\n",
    "    char_count = sorted(char_count.items(), key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    sorted_chars = [char for char, _ in char_count]\n",
    "    print(sorted_chars)\n",
    "    char_to_idx = {char: index for index, char in enumerate(sorted_chars)}\n",
    "    idx_to_char = {V: k for k, V in char_to_idx.items()}\n",
    "    sequence = np.array([char_to_idx[char] for char in text_sample])\n",
    "    return sequence, char_to_idx, idx_to_char\n",
    "sequence, char_to_idx, idx_to_char = text_to_seq(syr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47146742",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def get_batch(sequence):\n",
    "    trains = []\n",
    "    targets = []\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        batch_start = np.random.randint(0, len(sequence) - SEQ_LEN)\n",
    "        chunk = sequence[batch_start:batch_start + SEQ_LEN]\n",
    "        train = torch.LongTensor(chunk[:-1]).view(-1, 1)\n",
    "        target = torch.LongTensor(chunk[1:]).view(-1, 1)\n",
    "        trains.append(train)\n",
    "        targets.append(target)\n",
    "    return torch.stack(trains, dim=0), torch.stack(targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b710926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, char_to_idx, idx_to_char, start_text='. ', prediction_len=100, temp=0.3):\n",
    "    hidden = model.init_hidden()\n",
    "    idx_input = [char_to_idx[char] for char in start_text]\n",
    "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)\n",
    "    predicted_text = start_text\n",
    "    \n",
    "    _, hidden = model(train, hidden)\n",
    "    \n",
    "    inp = train[-1].view(-1, 1, 1)\n",
    "    \n",
    "    for i in range(prediction_len):\n",
    "        output, hidden = model(inp.to(device), hidden)\n",
    "        output_logits = output.cpu().data.view(-1)\n",
    "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()\n",
    "        top_index = np.random.choice(len(char_to_idx), p=p_next)\n",
    "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)\n",
    "        predicted_char = idx_to_char[top_index]\n",
    "        predicted_text += predicted_char\n",
    "    \n",
    "    return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e90855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.n_layers)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.input_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.encoder(x).squeeze(2)\n",
    "        out, (ht1, ct1) = self.lstm(x, hidden)\n",
    "        out = self.dropout(out)\n",
    "        x = self.fc(out)\n",
    "        return x, (ht1, ct1)\n",
    "        \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(self.n_layers, batch_size, \\\n",
    "                            self.hidden_size, requires_grad=True).to(device),\\\n",
    "                torch.zeros(self.n_layers,\\\n",
    "                            batch_size, self.hidden_size, requires_grad=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3c3c5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.2073133182525635\n",
      ".  о е  т  е е  ооо    о  оое  ае о о    аоои си  саоиа оот о  тое а  е оп , е оо  охе сол  о  от т л \n",
      "Loss: 2.9552474498748778\n",
      ". то зомо но па но тето ма то  то мо то бе ко  на пе то но суво не тано то наес та посто бо те кот то \n",
      "Loss: 2.71175669670105\n",
      ". тасти када косток но ос не пастори но вото коде оранно ни перто воне мак но пето ите вото сти пимо и\n",
      "Loss: 2.574607572555542\n",
      ". стодно дать постенно вода на постот стание в костам но  стер в пана наламо межно стертести постота б\n",
      "Loss: 2.493344306945801\n",
      ". совот но в кальво сторет стода не просто не в партави сторами помирот и стости посторивать слодить о\n",
      "Loss: 2.4262530517578127\n",
      ". стести старить на не проне не в и остение не то метик поронной все нально прокольвот провести на на \n",
      "Loss: 2.3632157325744627\n",
      ". стастевить не торить просто не в стода просто в не мерить просто продести страстви продильно просто \n",
      "Loss: 2.321212682723999\n",
      ". не на не стоже не и не споворно то в тока достверно вок то все не потот стодов страритини не на в ко\n",
      "Loss: 2.263089475631714\n",
      ". подверно поророль подоле просто он помормопорово свосто и прододельтать и воворе он подрости поства \n",
      "Loss: 2.2282214260101316\n",
      ". понерать как подмость в мурывает на поненной вомовать в том это не потом с тевестовать пожет старать\n",
      "Loss: 2.169735803604126\n",
      ". на в смодном на подобору подобном в одного от так в мерно присторной в просто не в морде и непоробно\n",
      "Loss: 2.157087697982788\n",
      ". тому это просто и да не от бустенно не в в подросновать не подостана не в потом не конататно не вот \n",
      "Loss: 2.1103188800811767\n",
      ". полутать, но не от простом стально был подронит не да играть потом не в смостого и подводить под под\n",
      "Loss: 2.0956679248809813\n",
      ". с на как то вот так может из из прододное страствие подобно в чем подновать подводововать выше на во\n",
      "Loss: 2.0428951358795167\n",
      ". просто не стравление сестевает просто продально не в том себе продавать не так можно с старой сумать\n",
      "Loss: 2.030995559692383\n",
      ". не просто не подобности на подстальный с корорые подмыс в подобной после построжения на получается п\n",
      "Loss: 1.9983900260925294\n",
      ". потому и самый вот подобно только не продовали не поднование просто как почему не почитать с собо по\n",
      "Loss: 1.9768402099609375\n",
      ". подобности да и от проводить себе не вот тогда в промерждения в под будет на после в том в просто в \n",
      "Loss: 1.9537895011901856\n",
      ". под под получить в подобности в постоя он подрадут после не вот вот не стравлятельности старования в\n",
      "Loss: 1.9426311874389648\n",
      ". не сторона просто так не подобного не только просто не от был подрозный пример вот просто не в прода\n",
      "Loss: 1.9297960376739502\n",
      ". больше собой игрок не подрости не просто не больший будет кончек не почему не стольной просто они не\n",
      "Loss: 1.8885680866241454\n",
      ". прододности на получает он не игрока на на собой просто всегда не вот как просто постоять в пример м\n",
      "Loss: 1.882287755012512\n",
      ". не не соблад собламнем пример и так и меня в постоять от так просто так получает понимает свот в соб\n",
      "Loss: 1.8569091033935547\n",
      ". не подтверждают от подобности у него просто не даже просто в нет продулято не они он вот в собой про\n",
      "Loss: 1.849389123916626\n",
      ". не все не такой просто не вот приворей просто не история в под про старой оборотния про постмодернин\n",
      "Loss: 1.8362620973587036\n",
      ". потому что так потому что как вот так из сам потому большости не потому сказать своими про сторона к\n",
      "Loss: 1.8306881952285767\n",
      ". продавать поддернуть не потому как подобнования и просто не как про собраться станативного может про\n",
      "Loss: 1.8171472644805908\n",
      ". подробности постани вот ты вот они он приведением себя не подставить контексте от господи, не постмо\n",
      "Loss: 1.8149216842651368\n",
      ". как не подобно подобности подлетание с про подробно с драма не столости не не потому постанспор с ко\n",
      "Loss: 1.7882133436203003\n",
      ". концекту на просто призового на столей привеление призовой и просто в симу стола на страна не просто\n",
      "Loss: 1.7849503231048585\n",
      ". под вот так не подобно не не просто так да не под немание оборотня не подобно подмотрел не не смешно\n",
      "Loss: 1.771695065498352\n",
      ". не вот это не подобном сказал в какой подобрать себя получить на постанет самое не собой подобрать н\n",
      "Loss: 1.7812118053436279\n",
      ". подобрал оборот в собрать, но не вообще просто ставление вот просто он просто с почему может в получ\n",
      "Loss: 1.7538384675979615\n",
      ". не вас и пропор больше только в концеп как то призовый стол, говорит на страна может быть привисить \n",
      "Loss: 1.755036334991455\n",
      ". потому что не понятно на подобности призовые и не быть не всё на сторона подобное получает может при\n",
      "Loss: 1.722541823387146\n",
      ". получится в получит в столе подобной становит с подруговорит подрывами призовой вот но то по станови\n",
      "Loss: 1.727419638633728\n",
      ". не будет больше не начала не вы только под старованный по сторона в собой и вампира и не вообще не п\n",
      "Loss: 1.7155983066558838\n",
      ". ведь про своими странное больше подобной просто блин столько и поставить и всё потому что пользовать\n",
      "Loss: 1.7213343286514282\n",
      ". по там проводить в постмодерной контексте в может быть в сталь на вот потому что в том получение соб\n",
      "Loss: 1.70230215549469\n",
      ". почему в переводить на подобного вообще не под тебя предложение с просто и про всё вообще потому что\n",
      "Loss: 1.6936129856109619\n",
      ". выше в пользу получить в совершенно самый игроков в какой просто в конце и подругей не выражение, но\n",
      "Loss: 1.6957503461837768\n",
      ". в игроков и столся и просто как про что у после в чем и не собой по больше с без вообще не получаетс\n",
      "Loss: 1.6895631074905395\n",
      ". с после и что в том и соблюдает как в вот в собликтивное с подобности с морских сторону за старого с\n",
      "Loss: 1.6852195596694945\n",
      ". от собой смыслета как просто смерти и тебя он вот то буривух потому что угодно было с сильно будет в\n",
      "Loss: 1.6714924192428589\n",
      ". с получить вот так да с попадать и всё на собрать на собой сеттинге не попасть не потому что вот как\n",
      "Loss: 1.6694657468795777\n",
      ". не просто под даже не с становить на стороны в постмодерн и про транспорт в какой не беспольза не по\n",
      "Loss: 1.6624456930160523\n",
      ". не возможность и просто не получить детей попали в пример про посмотреть на стоит в контексте на ста\n",
      "Loss: 1.6624994564056397\n",
      ". подобновали и себя просто не так собственного просто и приведевать можно в себе в сторона да старым \n",
      "Loss: 1.647951717376709\n",
      ". стоит потому что подобного потому что он есть про стоит просто всё остальной своих как в смысле пото\n",
      "Loss: 1.6478003644943238\n",
      ". в про тебя про самом пример всегда и подобнование только хороший подвотования на себе стоит да стрел\n",
      "Loss: 1.6530701541900634\n",
      ". страны потому что не просто не вопрос про подобно а страновку на стол постмодернисткая просто не как\n",
      "Loss: 1.6448511743545533\n",
      ". потому что так после как вот вот в ногу конечно, что он не будет сложный по образовать по представит\n",
      "Loss: 1.6191296768188477\n",
      ". получиться в контексте в налом с наличестве написал на вот в кино не возможность может как подобност\n",
      "Loss: 1.6346768712997437\n",
      ". то просто не в стороны с него не подставление с старый в старо как тупо полову не подробной посмотре\n",
      "Loss: 1.6143173551559449\n",
      ". ну вот только странит и под поставить на просто потому что не по столько не может быть в конце потом\n",
      "Loss: 1.6277576780319214\n",
      ". стол с транспорт и не потому что не стороны на привильно в смысле не вот так он привель не вампира в\n",
      "Loss: 1.620508346557617\n",
      ". с получиться на старой подразумевается в своим просто только под него не собрать и про хорошего в по\n",
      "Loss: 1.6205168390274047\n",
      ". не сказать написано это не получится с ним не придётся на стороны на стороны не про все стороны и по\n",
      "Loss: 1.6175015544891358\n",
      ". в том и все и соблюдона в мире странности с него и тоже не подоров в мире просто и продолжать на ста\n",
      "Loss: 1.6072666025161744\n",
      ". в постановкой сторону с пидорбот так и не собой на соборка получится на стороны надо с пользовать пр\n",
      "Loss: 1.5941900157928466\n",
      ". потому что так подобности и получается на может конечно с после вообще в попросить вот это не про вс\n",
      "Loss: 1.5914050579071044\n",
      ". не будет как не вопрос не просто вопрос подобите у него как не был на придётся просто про старый при\n",
      "Loss: 1.5873068046569825\n",
      ". получается на стороны не подняться человек в пост просто с при чем тут подходит под просто столько н\n",
      "Loss: 1.5843217706680297\n",
      ". всё ваших подобных под дело в просто стануть написано кондитерской и стол подводков собственному про\n",
      "Loss: 1.588957805633545\n",
      ". почему не вопрос как не в просто с подобной себя после совершенно с призовой сторону так вот это как\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.582492346763611\n",
      ". потому как он не собственной и пример и подобной больше конце стороны образование тебе подобное оруж\n",
      "Loss: 1.5746820640563965\n",
      ". конечно, но вот под давать в прикол потому что в питер мне вы потому что потому что по сторону в пол\n",
      "Loss: 1.586758074760437\n",
      ". наматических вопрос просто потому что так так он вот вот только не был то не столе не подходящем не \n",
      "Loss: 1.5633308458328248\n",
      ". слова просто то потому что не как понятно подруги просто пример с то становится в собака и круто не \n",
      "Loss: 1.5503643894195556\n",
      ". не вот это выше придь как вот в контексте в себе просто было возможность и выступление в подможно из\n",
      "Loss: 1.5501693105697631\n",
      ". не привет и подмостки и такой только не под просто в подобных создать на более про вообще под конце \n",
      "Loss: 1.5519890451431275\n",
      ". потому что в отношение в результат подразумевает просто просто с просто переставить просто не вопрос\n",
      "Loss: 1.5513628149032592\n",
      ". почему не в просто и общения и подруга в то и одного пример просто так просто можно постмодернист и \n",
      "Loss: 1.5637082242965699\n",
      ". потому что он не на собой с после ваши результат состояния после просто так вы не было подробности и\n",
      "Loss: 1.547859125137329\n",
      ". как на по разному в своей как вот можно вот в чате в мертва не прикольно сказать потому что в тем не\n",
      "Loss: 1.5494242238998412\n",
      ". потому что про все и не надо на подобность и даже не вообще не просто не вопрос получается на странн\n",
      "Loss: 1.547416615486145\n",
      ". почему тогда про подругие статьи подходит на своим не задумался потому что вообще не было развитие о\n",
      "Loss: 1.5461158132553101\n",
      ". в комнате не продолжения может быть вот в своей приведи и прикол понимается на столеха не пример про\n",
      "Loss: 1.5384978103637694\n",
      ". странные сторону именно в буку за контекством и не другой доставать на просто не просто не полнока в\n",
      "Loss: 1.5346366357803345\n",
      ". в остальное просто не всем столом с него с него сопросить просто не после контексте в совершенно оби\n",
      "Loss: 1.5329154920578003\n",
      ". потом в общественной силовика в подобности в серьёзное секс это в подобного получиться в контекст с \n",
      "Loss: 1.5394095230102538\n",
      ". в контексте в него уже не потеря и получился с призовые борд не получится на сторону как больше подр\n",
      "Loss: 1.5307716369628905\n",
      ". как вы раз под призового меч просто способность подразумевает в смысле постмодернисткие и постмодерн\n",
      "Loss: 1.522895245552063\n",
      ". так и призовые больше про призовой собой про все имеется в свою под стороны не больше не подобной ка\n",
      "Loss: 1.5172943210601806\n",
      ". потому что не просто не прикола в пользу не про стрелят как опять прикол подробное столько с может н\n",
      "Loss: 1.538105001449585\n",
      ". старой в смысле после просто так там не потому что так и получится на стороны не собственность после\n",
      "Loss: 1.5180021572113036\n",
      ". не всегда просто не получится, но в том в сладкой концепцию от транспорт в том а не спорт но не было\n",
      "Loss: 1.508021683692932\n",
      ". на более от призовой раз по понимает на настольно против и призовой мировоззрение достоменно не приз\n",
      "Loss: 1.515048451423645\n",
      ". том стол, но вот только с полноценный и про этого не подвод просто не стоит свои не меч в просто стр\n",
      "Loss: 1.5293882846832276\n",
      ". остальные своих по сторону подруга, но вот так себе крови с собой за потому что под собой просто не \n",
      "Loss: 1.5141299533843995\n",
      ". не про стол с разный просто всё ещё и просто не заводить на прикол, который выше на добавить, на сто\n",
      "Loss: 1.5179655790328979\n",
      ". ну с собой и про вопрос поставить порганистовал и получается на вы подмостично просто старой под ста\n",
      "Loss: 1.5073125457763672\n",
      ". ну не быть меня в последние верный сладкой стволов не значит он обоснования в своих сторону на полов\n",
      "Loss: 1.5025755310058593\n",
      ". старого в пользу не становится на разные и сказать на свою подругой просто не просто не призовой сме\n",
      "Loss: 1.5004109859466552\n",
      ". получается на чем тут не смерти с него своими сторону у нас бы не стоит на сторону не начало просто \n",
      "Loss: 1.5029199314117432\n",
      ". а не вывозят ловкости просто стоит просто и по столик самостоятельности только про полноценный успех\n",
      "Loss: 1.4988301038742065\n",
      ". не под собственность на столе возрастит конь не просто на столом и не просто с непонятно и не слова \n",
      "Loss: 1.5016347217559813\n",
      ". борьбе подобности играть потому что в комплексов, но понятно потому что попадать по вообще не ответ \n",
      "Loss: 1.4911131238937378\n",
      ". старом и просто с потому что не стоит собрать на сторону под просто не выступаешь в последнего прост\n",
      "Loss: 1.4975577878952027\n",
      ". будет не становится и вообще, но в чём тут даже собственность и даже не в подобной произведении в то\n",
      "Loss: 1.5054971027374267\n",
      ". стороны не просто с место и приколов и прикольный как не в собственность про весь из за соблазнение \n",
      "Loss: 1.4902567672729492\n",
      ". как просто специализации в первой стойло в ролевой собственность конце минут только собственность по\n",
      "Loss: 1.4806568384170533\n",
      ". так и потому что подразумевает на стража потом не отношения от признание подробнее просто в обществе\n",
      "Loss: 1.4868330192565917\n",
      ". на старой всем вампира можно и подруга про подобной собственность просто с получилось игроков в свои\n",
      "Loss: 1.4879517269134521\n",
      ". старой старой просто тебя по развития стол, в контексте вопрос получить получить в конце не подставл\n",
      "Loss: 1.4901845741271973\n",
      ". смерти из другой случае про ну в основной и не в процессе и понимаю не современно с приколы с просто\n",
      "Loss: 1.4748299026489258\n",
      ". как бы почему не то не стоит под поменяй, конечно, понимаю на просто не надо просто не подруга надо \n",
      "Loss: 1.4838046026229859\n",
      ". старовано по страка метак в смысле, после вампир как не подставляется с понимаю в частности в постмо\n",
      "Loss: 1.481577115058899\n",
      ". вопрос по стол как половить после совершенно потому что в марат и подобность под собой столем по пол\n",
      "Loss: 1.476709156036377\n",
      ". как подобное сказал в конце отыгрывает столе и как оно не подобно с ними не больше странное стол в б\n",
      "Loss: 1.4795700788497925\n",
      ". как вот это не в пользуюсь в сладстный подобных становится на своих просто не важно так в средневеко\n",
      "Loss: 1.4924070310592652\n",
      ". на столе в треба на постмодернистких гей не подходит на столом, но вот как про одно и игрокам для со\n",
      "Loss: 1.4770869636535644\n",
      "Epoch 00113: reducing learning rate of group 0 to 5.0000e-03.\n",
      ". а как на стол в произведение про столом про столом и понятно не под собой как и старого как скорости\n",
      "Loss: 1.4462786865234376\n",
      ". призовой бой и даже не прикольно признак в принципе, не все такие в том, что соблюден снова как вы п\n",
      "Loss: 1.4206526470184326\n",
      ". почему только на просто не столом выступает за всем сам получилось по старовать и понятно это больше\n",
      "Loss: 1.4131191873550415\n",
      ". потому что он и просто и вообще и так потом в постановка на пользоваться в пользу старой тем более о\n",
      "Loss: 1.401302318572998\n",
      ". стоит с собой в подобных получился в подводку сильно своим соблазний на столько и представить под по\n",
      "Loss: 1.3879815530776978\n",
      ". с компетенции не известно сказать на вероятности и всё такое доспеха а он вообще не потому что не пр\n",
      "Loss: 1.3713132429122925\n",
      ". не будет слова подругой про вопрос в процессе не в столика в контексте игра в своём не может быть пр\n",
      "Loss: 1.373845615386963\n",
      ". не столе с него про вампира достаточно просто в собственность просто просто с потенция просто так та\n",
      "Loss: 1.3684714078903197\n",
      ". потому что вот только не оборотни и просто сталь на странные просто в подобной сладкого под собой пр\n",
      "Loss: 1.3612362766265869\n",
      ". так они после столе тебе успех не подобно не про точки от может быть концепта не подобно с ним за пр\n",
      "Loss: 1.3451037120819092\n",
      ". какой стороны, но просто не надо так так это не то понимаю на сторону и просто способность и потому \n",
      "Loss: 1.350105972290039\n",
      ". какой прикол по попробовать, в том и понятно под конец, в старый подруга про признание как вы просто\n",
      "Loss: 1.367459363937378\n",
      ". не подруги, но вот вам не то не подвидитель просто так они просто не подругой совершенно не в сторон\n",
      "Loss: 1.3302885961532593\n",
      ". потому что это не подразумевает силой конкретно и просто не стоит на столько с произведениях выражен\n",
      "Loss: 1.330342116355896\n",
      ". не сказать на процессе всё на столе не вампира с получить в смысле, вашей полноценный стороны потому\n",
      "Loss: 1.3380612468719482\n",
      ". потому что было в процессе разного принять прикол по факту про подставить как он не всегда по первог\n",
      "Loss: 1.3396014881134033\n",
      ". не подобный выбор с контексте возможность подразумевает в процессе в своих под бабки и стоит постави\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.34477126121521\n",
      ". это в процессе не понимания подобные мальчика на вообще потому что только в смысле, но в пользу не п\n",
      "Loss: 1.3277104425430297\n",
      ". старого концепцию сраную сторону не получается будет совершенно не стоит старом играть не в старым д\n",
      "Loss: 1.3259452676773071\n",
      ". не вопрос а совершенно не вопрос в конце ты если ты возможность и сложность и всё больше отравления \n",
      "Loss: 1.3286802196502685\n",
      ". потому что это показался про призовые бои не вопрос а не подобной подходит в собственной стороны по \n",
      "Loss: 1.3142390060424805\n",
      ". на прикол про подобной стороны на привет для подписывал на старым получился в собственного кроссовер\n",
      "Loss: 1.31782723903656\n",
      ". не просто с начала с ним это постановки на стрельбу с ним за принципиально сильные призовой стороны \n",
      "Loss: 1.3104320526123048\n",
      ". собрать не было бы как бы не после стороны понимаю и того, что вы так попадает в том и транспорт от \n",
      "Loss: 1.3122427892684936\n",
      ". почему после больше в день забавно по вопрос возбуждающее откровенны и почему и просто с написано в \n",
      "Loss: 1.309367060661316\n",
      ". какой стол с больше своих испортить выше про человека получить из за стола просто не подробнее в смы\n",
      "Loss: 1.311640419960022\n",
      ". потом привет собрались с пользования смотрель, но в каком про вампиров персонажа получается так сказ\n",
      "Loss: 1.3046541261672973\n",
      ". в процессе не вообще не пример после выстрела с признание на столе как по тактика с собой не просто \n",
      "Loss: 1.3003932094573976\n",
      ". ставит про них старовиль давайте сладкой сторону под другими записывать в конце манипулировать больш\n",
      "Loss: 1.3087489414215088\n",
      ". а не сможет подходит от играть но вот это пример в кармане по старом смысла личности под собой конкр\n",
      "Loss: 1.2977019214630128\n",
      ". не получится, в своих персонажа под стол в старый вопрос по полезное отношения с камнем вы такое тре\n",
      "Loss: 1.308115849494934\n",
      ". почему так в чем оттуда? так то как он не понимаю но просто из получить в неверный призовые бои не с\n",
      "Loss: 1.300688509941101\n",
      ". или подобного после подмостки и просто не нужно так не понятно а не по рукам или говорится про вампи\n",
      "Loss: 1.3062903022766112\n",
      ". какой конечно доставать после травма в конце под концепцию концепция подводочку с постановка на стол\n",
      "Loss: 1.2986473178863525\n",
      ". потом не стоит коллекции и всегда по получилось по получить частью в старого под собой постановка ни\n",
      "Loss: 1.2924392604827881\n",
      ". только себя совершенно не просто слова частью выше он не было и со своим и просто для подобного стра\n",
      "Loss: 1.2866755771636962\n",
      ". не подобности и не просто с конце не сможет от него разница в смысле, но собственность и стать не по\n",
      "Loss: 1.294806261062622\n",
      ". сторон кончике на пользоваться со своим играть на пользоваться в процессе в конце не все равно стоит\n",
      "Loss: 1.2863463830947877\n",
      ". так это всё на просто в него споренивая в городов с передачи открыть в пустотой и против просто с от\n",
      "Loss: 1.2847370624542236\n",
      ". ах, как будто подходит от слова просто с собой после всех в смысле, но там понятно на месте просто с\n",
      "Loss: 1.2902328300476074\n",
      ". не вопрос подразумевает средневековье про потерять. но в требую подразумевает в собственной раз не б\n",
      "Loss: 1.2769965505599976\n",
      ". потому что там попадает в сеттинге транспорт как вы состояние на стол в старых постмодернист и не мо\n",
      "Loss: 1.2899773359298705\n",
      ". не в сторону под собой тебе привет для признание в свою просто своих ответ в отсутствие по вопрос мо\n",
      "Loss: 1.2766253185272216\n",
      ". потому что подробнее в процессе. там вообще не понимаю но это просто с насильного придумать не было \n",
      "Loss: 1.2801759481430053\n",
      ". то не самосбор в постоянно подрядь в мире тьмы поражение про столом и потому что так это всё такое о\n",
      "Loss: 1.2735031604766847\n",
      ". в попробовать на подробностей, но в чате ваши подробнее в своём игроков после вашем история на произ\n",
      "Loss: 1.2737589073181153\n",
      ". в том, что вы специалист после волоса по столом и попадать не подождите за подсказки в контексте пре\n",
      "Loss: 1.268910903930664\n",
      ". потому что вот так так он не совершенно понимаю но вот просто обоснование смотреть по правильно возм\n",
      "Loss: 1.273935832977295\n",
      ". а также не сломает в старого собственность метания от силы и подруги с контексте старого странно мог\n",
      "Loss: 1.2674590969085693\n",
      ". потому что не всегда и морти и потеря на ваших подобной повод вот только в столярной мастерской имен\n",
      "Loss: 1.271728410720825\n",
      ". вопрос по столица в компетенция только спортивных общественного контекста под собой себе на производ\n",
      "Loss: 1.260587501525879\n",
      ". то не задумано про сторону не проблема в процессе. не было про точки на вопрос можно подробнее в себ\n",
      "Loss: 1.2567228698730468\n",
      ". конечно быть старой возможность под большим в собственного про них напряги, на просто не вопросу не \n",
      "Loss: 1.2519950580596924\n",
      ". почему бы вот это в старый прикол про сеттинг под собака что он просто так и подругой в процессе воо\n",
      "Loss: 1.2529365491867066\n",
      ". кому как постмодернист на страшный, но в том и зависимый странно обоснование постмодернисткий больше\n",
      "Loss: 1.2577383518218994\n",
      ". старала в чате вампиры в контексте старых подобных испортить в сторону в смысле, в доброго произведе\n",
      "Loss: 1.2573724269866944\n",
      ". потом сабля что? по получается и вообще можно получился на пользуюсь как на половую серьёзное про на\n",
      "Loss: 1.253538498878479\n",
      ". в процессе не надо получилось с которой и не подходит в конце не всегда разному вернуть на подробнос\n",
      "Loss: 1.2542056941986084\n",
      ". а, про всех от тебя не получится, но в смысле, как подобными вопрос откровенные возможно по стороны \n",
      "Loss: 1.2478157234191896\n",
      ". собственность по вопросы не в зависимости, у меня в том, что ты совершенно по призового будет в голо\n",
      "Loss: 1.2505362462997436\n",
      ". потом стрелят в подобное от призовые бои и просто просто столько может быть на пользоваться на месте\n",
      "Loss: 1.2513216018676758\n",
      ". потом стрелять собственные слова на стрелять как не стоит на страшный, но в театральные подруга. но \n",
      "Loss: 1.2455451250076295\n",
      ". потом стоит в пользу собственность и про вампирам и вообще уже просто не подрывает в раза это под бо\n",
      "Loss: 1.2496092128753662\n",
      ". подобно с полных сограть в сеттинге меня в сторону подразумевает супер стрельбу но в третий стоит пр\n",
      "Loss: 1.2444822597503662\n",
      ". не значит написал он не будет слова в пользу собственной стороны просто придётся как про возможность\n",
      "Loss: 1.2333522748947143\n",
      ". не подразумевает в процессе в транспорт ли в процессе не потеря подруга. но потому что такой это не \n",
      "Loss: 1.2444118070602417\n",
      ". ставит от правила с получить в собственного продолжай, но потом они призовые бои как то же понимаю н\n",
      "Loss: 1.232076177597046\n",
      ". старой и всё конечно, в процессе отсылка на старо не всегда и не подводочку по получается с ног можн\n",
      "Loss: 1.2263211727142334\n",
      ". не вопрос можно и стрельбу и всё на вахе, ну вот если ты сколько спорт и просто и проверять игру и п\n",
      "Loss: 1.2340481567382813\n",
      ". не ответ был про призовые бои в процессе не навыки на него в контексте не подруга на принципе можно \n",
      "Loss: 1.2259332036972046\n",
      ". не под собой и подруги по старой кубы на этого не было от тематика на подробнее про призовые бои как\n",
      "Loss: 1.2336991834640503\n",
      ". не просто так так это не было в сторону и про пользоваться на производство не подруги. но ты сложнос\n",
      "Loss: 1.2253607130050659\n",
      ". потом сладкий чата так они не получится конечностями под столом, который призовой боец это не подруг\n",
      "Loss: 1.2376177358627318\n",
      ". вообще, ибо по старой кубы на просто странность и произведениях вопрос открыто обоснование в организ\n",
      "Loss: 1.2269995355606078\n",
      ". потому что по сердированный. но в своём раз обосраться конкретики в процессе в пользу нет и не подро\n",
      "Loss: 1.2252417182922364\n",
      ". под большим в старый привет большая просто просто столько не подрыв с пользуюсь потому что вы вообще\n",
      "Loss: 1.2183808946609498\n",
      ". в принципе не опросу от пользования как просто с собой то не понимаю но по причине про после всего в\n",
      "Loss: 1.2278425550460816\n",
      ". стопроц не могу не подробности и стать и просто не понимаю но в процессе. в том, что вот это просто \n",
      "Loss: 1.212226448059082\n",
      ". старой и специальный выпад на сейчас почему так это не может быть компьютерная кармана потом и приве\n",
      "Loss: 1.2216178941726685\n",
      ". попасть в своём ролевые игры в том и отыгрыша, но вот это не вопрос получается, но вот это же подраз\n",
      "Loss: 1.222770824432373\n",
      ". потом это не подрывает на стрельбу из количеством про какой просто с пользования конфликтный констру\n",
      "Loss: 1.219647264480591\n",
      ". не всегда ещё в первой призового бойца ахахахахаха просто не надо стоит в процессе не подразумевает \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2068469095230103\n",
      ". не вопрос в контексте полноценный подводку в полного рассказ и призовой боец для подмостки, потому ч\n",
      "Loss: 1.2039357376098634\n",
      ". не вообще не говорить доставить от компенс в сторону просто не надо старый большой раз по причине эт\n",
      "Loss: 1.202080202102661\n",
      ". старых под тебя не подходит на перевод можно до первого в столярной мастурбация по получается убил ч\n",
      "Loss: 1.212206392288208\n",
      ". пользуемысь и приколы так просто потому что потому что это не подводки над подразумевает старый голо\n",
      "Loss: 1.1989238119125367\n",
      ". как бы игра в собственном собственной игры и был от собой как возможно, в том, что он не просто слож\n",
      "Loss: 1.204605884552002\n",
      ". не подразумевает в своём решение на всех выбор скажем, будут троичные про подводки и всего на пользу\n",
      "Loss: 1.2172578954696656\n",
      ". не буду не стоит под будет в пользу своих голосом, ну так ты крови в процессе не понимаешь, что в то\n",
      "Loss: 1.2024541473388672\n",
      ". не понятно не спорта в рот в пользу тебя подразумевает в логово не ответил от тематика на стрельбу и\n",
      "Loss: 1.204480710029602\n",
      ". потому что вот это всё почему потерян из коммуникация и потому что вот просто с ним по положить пост\n",
      "Loss: 1.1956985759735108\n",
      ". не подруга на половка хорошая под ролевые игры получится как подобной конструкции. а если нет почему\n",
      "Loss: 1.1894539451599122\n",
      ". не подразумевает становится возможно, в травмы с собой для персонажа и просто так не смотрел возможн\n",
      "Loss: 1.2015158653259277\n",
      ". не буду просто не стоит под собой под столом, но вот это не показатели собок посмотрел собака про пр\n",
      "Loss: 1.1949692487716674\n",
      ". почему по старой про них собака не получится, в том, что так он не подружками, а там на полном персо\n",
      "Loss: 1.1985034370422363\n",
      ". не ответил, чтобы ответ не подругой мировой старательности с собой по столом, например, на старом см\n",
      "Loss: 1.1940962266921997\n",
      ". собаку? все стоит при чём тут под принято на пользоваться на стрелят от призовые бои, а также в поль\n",
      "Loss: 1.195652084350586\n",
      ". потом стройство потом а как будто получилось больше сильно другой стол, конечно так просто строка са\n",
      "Loss: 1.1924580574035644\n",
      "Epoch 00211: reducing learning rate of group 0 to 2.5000e-03.\n",
      ".  контакта ловкость почитать в пользу собственность по старо и почему так там не всегда в произмечает\n",
      "Loss: 1.1695212364196776\n",
      ". пользуясь к большинство под собой. собственность и просто всех игрока в становился на пользуюсь степ\n",
      "Loss: 1.1448493099212647\n",
      ". долг человек в пользу своими недостатки в подоблей, от тебя не просто не выступаешь даже не подробне\n",
      "Loss: 1.1271384763717651\n",
      ". не ответ не понимаю но в своём подобной городике как под тебя делает конечно, но они не понятно но с\n",
      "Loss: 1.1161160039901734\n",
      ". пользуешь, например в процессе не понимаю но там не призовые бои как про самое известный ну, почему \n",
      "Loss: 1.109382939338684\n",
      ". становится как просто и всё равно постмодерн в мире отсутствие это что вы про господи, а не привет с\n",
      "Loss: 1.1065634632110595\n",
      ". не понятно но в сторону и не согласится с диалоги новыми челюсти привлекательности под мастер может \n",
      "Loss: 1.0912108469009398\n",
      ". не подругой подобному в подобных без полного оружие в процессе и потому что в стол, но в чём там так\n",
      "Loss: 1.0931587743759155\n",
      ". не ведь не открывается на стрельбу в принципе не подруга и всё подробнее не получилось в смысле, но \n",
      "Loss: 1.0978731536865234\n",
      ". не бой про гроб из заметы по вопросы в контексте призовые бои как про кондитерского произведения, но\n",
      "Loss: 1.0835078525543214\n",
      ". потом не подводочку, в какой стороны, который все круто просто стрельбу линский странно полноценный \n",
      "Loss: 1.0868722009658813\n",
      ". не особо стоит на это и прочим можно подружка. на старом смешно по стелсу? это не было и страны за с\n",
      "Loss: 1.0870252275466918\n",
      ". не видел нормальной конструкции тем более что вот это же только про одной выбор из сути как да, это \n",
      "Loss: 1.079266800880432\n",
      ". боряться на пример с него сторону не подходит от кринж и не проблема и приколов. подробностей, ну пр\n",
      "Loss: 1.0765410804748534\n",
      ". не подобной получить и прочее. а не понятно по теме всё такое должно быть про стрельбу и специализац\n",
      "Loss: 1.0801101207733155\n",
      ". потом не добавить конечно, со всех разница в своей игры и смешно среднего человека не понятно по пер\n",
      "Loss: 1.0677967548370362\n",
      ". или подразумевает в свою возможность и он так он не понимаю но просто не может быть концепта ничего \n",
      "Loss: 1.0705795001983642\n",
      ". так это всё подразумевает в смысле под столом, смотрит с полсона нас страны понять можно подразумева\n",
      "Loss: 1.0687367916107178\n",
      ". под пользуюсь с борьбе просто с ним не прикол просто свободный секс с постановки на производстве не \n",
      "Loss: 1.0626361465454102\n",
      ". так может быть своим подробнее понятно, что они не защитить своих выше по водить постмодернизм. пост\n",
      "Loss: 1.0686780548095702\n",
      ". попасть, наступить, потому что совершенно не было не подходит возможно, был действительно стоит под \n",
      "Loss: 1.0599475860595704\n",
      ". или там есть под персонажа стрельбу из комуто не стоит в смысле может быть концепцию получился по по\n",
      "Loss: 1.0551010942459107\n",
      ". потом слишком много понимание с призовые бои не всегда по теме с ним не вызывает в смысле, это же пр\n",
      "Loss: 1.057946662902832\n",
      ". пользуйся и прочие про мальчика с собой пример сторон в моём и портрет в принципе с нормально пробле\n",
      "Loss: 1.0508297300338745\n",
      ". не понимаю но не совершенно не понятно в столярной мастерской и прочие под конец. это всё слил меня \n",
      "Loss: 1.0533291053771974\n",
      ". почему нет мастера а не понятно, что это не получился по канону, потому что в процессе в мастерской \n",
      "Loss: 1.0445070743560791\n",
      ". потому что мышлением старым подобному в собственной стороны, но как бы он по вопросы и получить на с\n",
      "Loss: 1.0528968286514282\n",
      ". не подразумевает себя по столом и не смог не проблемы в старать под собой просто не стоит стравмости\n",
      "Loss: 1.0480338668823241\n",
      ". не подразумевает выбор с ними в то кармана не было, это не получится? это не вопрос получается убеди\n",
      "Loss: 1.0447629690170288\n",
      ". стануть к ним прикол произведениях и потому что посмотрел в процессе. не понимаю но вот это всё оста\n",
      "Loss: 1.0391390538215637\n",
      ". под собака на стрельбу произведения и потому что не подходит как даже не вопрос мостом так как прост\n",
      "Loss: 1.0394054937362671\n",
      ". от понимаю ну это же приколов и тема в том, что это всё ещё классических движения получится к собой?\n",
      "Loss: 1.0335830903053285\n",
      ". не имеет отвечает человека и подробности с ним призовой боец это надо рассказывать про страшные подр\n",
      "Loss: 1.0341387009620666\n",
      ". не вопрос можно потерял совершенно очевидно было возможность и прочего отыгрывать может как про само\n",
      "Loss: 1.0411724305152894\n",
      ". под соблюда можно поставить сильно другой информации? и получится, вот так просто выставляется, но в\n",
      "Loss: 1.0369485855102538\n",
      ". потом страшный, а некоторые даже не подождите человек в одном из транспорт как просто так это вещь п\n",
      "Loss: 1.0397470331192016\n",
      ". не постмодернист оружие и получился потому что они просто так с принет такой концепты и должно быть \n",
      "Loss: 1.0359937810897828\n",
      ". так это будет вообще просто сложная, что они под собой под собой страны. вот выдвижение с ним не про\n",
      "Loss: 1.0312528610229492\n",
      ". не ответ быть под собой под собака и подруги. оно сменяются ножа и стать не подруга на половерной ст\n",
      "Loss: 1.0328078508377074\n",
      ". не подразумевает в смысле, они реально не становится в оригинале надо по рукам да, просто себе на во\n",
      "Loss: 1.0263350415229797\n",
      ". не подходит как просто так это в процессе или в стол с борт делают в процессе не совершенно не получ\n",
      "Loss: 1.0350802946090698\n",
      ". а, ну да намажными от собака в процессе а сыркова. нет не согласится с ним секс страдать от тебя же \n",
      "Loss: 1.0305534887313843\n",
      ". не возможность про старых день! к совершенно не высокой конец. это надо страшный можно по получить в\n",
      "Loss: 1.0282938766479492\n",
      ". под контексте стреляют как то как будто в столярной мастерской получается только рассказывает. потом\n",
      "Loss: 1.0203271055221557\n",
      ". не подруга и почему так вот это не под который про собственность попасть, на старом смысла с конца. \n",
      "Loss: 1.0237941455841064\n",
      ". не подразумевает вес море всем выпали на точно сломать в первом сексуальной конструкции и самоиронию\n",
      "Loss: 1.026136691570282\n",
      ". потом в смысле как в контексте решение почему не подставляю вопрос не подобными из страшный, но вот \n",
      "Loss: 1.0237960863113402\n",
      ". не вопрос в мире тьмы не полноценный прикольно привет держать в смысле красноречие в контексте согла\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.024605622291565\n",
      ". хаха, да в то проблема и подруги. но не понял, что он не подруга и подожди говорить про который жест\n",
      "Loss: 1.0234712481498718\n",
      ". не достойные подробнее может быть крови в том, что такое ответить, что они не показался на стрельбу.\n",
      "Loss: 1.0146697497367858\n",
      ". не понимаю но с мешает на пользования мастер не подходит от просто с собой так то и всё равно подроб\n",
      "Loss: 1.0181203484535217\n",
      ". да он может быть собой не против образное оружие и без продолжаешь но в собственности в середине све\n",
      "Loss: 1.0089677023887633\n",
      ". не видел в смысле как будто в стол с ним не подрыв в бар уже не подразумевает верным на самом деле, \n",
      "Loss: 1.017807388305664\n",
      ". потом старым и прочитать кондитерского изделия в старый просто не особо не подрыв с ним не подвид в \n",
      "Loss: 1.0183887243270875\n",
      ". собственность про конструкции. так сказать на говорится из за сторону в подобных произведением. и во\n",
      "Loss: 1.0107477045059203\n",
      ". порочности, конечно открывания как в процессе аргументация, в баре и проблемы в процессе. и тот соба\n",
      "Loss: 1.006238124370575\n",
      ". деле и прочее. в процессе. не получилось по правилам или реально обсуждаем отыгрывать в столярной ма\n",
      "Loss: 1.011757333278656\n",
      ". там собственность и привет для половую игра а сырком? не подразумевает в смысле, получается да и тож\n",
      "Loss: 0.9968570160865784\n",
      ". старой в плане от подставить собак посмотреть на рассказываю это просто с конца, конечно хорошо, что\n",
      "Loss: 1.0067056035995483\n",
      ". в пантальным ну скорее подруга а сколько больше смысла конечно получить из который вы старых руками \n",
      "Loss: 1.0079252099990845\n",
      ". пользуйся в смысле непонятно так просто так может быть как можно понять ты не просто с количеством н\n",
      "Loss: 1.0038181805610658\n",
      ". так это не подводку деньги и не ответил не стоит произведением по вампирам и приколов. потому что ты\n",
      "Loss: 0.998645920753479\n",
      ". не подруга для призовые бои, это не должны подразумевает в первую лица не про столка или подставить \n",
      "Loss: 1.0002218103408813\n",
      ". не весь постановки на столкал из способа нет, скажение признают зависимости и подруги. но вот это не\n",
      "Loss: 0.9983136320114135\n",
      "Epoch 00275: reducing learning rate of group 0 to 1.2500e-03.\n",
      ". расскажите образования собака не понятно по сюжету что не как признак по вампирам и прочего и просто\n",
      "Loss: 0.986484055519104\n",
      ". там большим и прочие ответственно потеря аргументация в процессе или короткий карман в средние верны\n",
      "Loss: 0.9593337464332581\n",
      ". не в своём результат у меня не стоит на уровне который вопрос понимать стараться достается не подвид\n",
      "Loss: 0.9579748916625976\n",
      ". не возможно, это в своих перевод с просто с коллективной мастурбацией, не видел блин, это не было во\n",
      "Loss: 0.9610325717926025\n",
      ". не вижу смысла интересная такое ответственность и прокатилась? как просто своими на вопрос в процесс\n",
      "Loss: 0.9527560639381408\n",
      ". боряться на ваших месте тут едят детей. смешно их в стол. это просто столом, собака не понял, что он\n",
      "Loss: 0.9512084436416626\n",
      ". собственность про них какие и он не могут старшего после волна так это не полноценный выбор из собст\n",
      "Loss: 0.9478808283805847\n",
      ". не ответ убил человек как мой на стрельбу и страны с конца. но ты сраный большой мастерство аторошей\n",
      "Loss: 0.9459930562973022\n",
      ". не в столярной мастерской или не надо самое время на сверх из волк а сладкий канал про полностью, но\n",
      "Loss: 0.9339938306808472\n",
      ". не вообще не подразумевает сугубо просто с собой как в первый игрок на разные подрыв на пользуюсност\n",
      "Loss: 0.9361100506782531\n",
      ". не понимаю. в собственной принято про страдания с ним секс самый просто так просто не могу либо реша\n",
      "Loss: 0.9268155646324158\n",
      ". не подразумевает в столярной мастерской и всего на сверх игрока постановки в своих за вопрос настоящ\n",
      "Loss: 0.9383563256263733\n",
      ". не возможно, это не понял. девочек для пидоров просто так не проблемы и прочим есть персонажа мало п\n",
      "Loss: 0.9274554419517517\n",
      ". не возможность после всего вообще не может быть как бы, никто не просто после волосами и прочих движ\n",
      "Loss: 0.9360389590263367\n",
      ". не понятно в принципе или как будто просто с добро все транспорт под стол. он про призовые бои решен\n",
      "Loss: 0.9282114315032959\n",
      ". не вопрос можно получиться на столе. использовать системы, чтоб конкретно просто не вопрос в вашей о\n",
      "Loss: 0.9262454557418823\n",
      ". под больше навалить из который сладкого просто не понимаете. а если нет минус про понятно и все или \n",
      "Loss: 0.9248950815200806\n",
      ". не постановка какие же это просто пример призовые бои как призовой боец это из заводной по столом и \n",
      "Loss: 0.9221607947349548\n",
      ". в отравления на производства не получилось с коллекционировании на ваших душу у меня нет конкретно п\n",
      "Loss: 0.9280772233009338\n",
      ". не подразумевает в рот получится, ведь не понимаю но не просто не всем согласился но скорее всего, м\n",
      "Loss: 0.9170875978469849\n",
      ". не понимаешь, что это всё такое достать? ну вот это просто перестанов с первого в сторону и даже не \n",
      "Loss: 0.9255543446540833\n",
      ". не получить из собственность постановки из смешно слова было бы не сможет словить конечно меч и прос\n",
      "Loss: 0.9248041224479675\n",
      ". а не понятно получается так не проблемы с постановки или кармана как просто стремится как проблема т\n",
      "Loss: 0.9186073136329651\n",
      ". не возможно, если ты судить на себя, конечно, в подмотрит странно себя за столом, скрытности и проче\n",
      "Loss: 0.9117968297004699\n",
      ". старовляют в смысле, примитивные подобной глазурью а как будто постмодернист и всё ещё в разного мор\n",
      "Loss: 0.9164729094505311\n",
      ". не понимаю но в своём результат попадать возможность и сложно. тогда про столом, ну да, но истина он\n",
      "Loss: 0.916606776714325\n",
      ". не понятно это же просто с контексте подрыв на стол с конца. не в сторону игра по факту просто так п\n",
      "Loss: 0.9102719020843506\n",
      ". от потому что в смысле, по рукам на самом должен быть про собственность попасть, но в процессе ли не\n",
      "Loss: 0.9165161871910095\n",
      ". не всем записывая возможность и подруги. но вот это в своём ролевые игры это просто с контексте отыг\n",
      "Loss: 0.9118102884292603\n",
      ". не возможность и страны и прочие от наличие игрока как подправить. ахах ахах, вопрос мой много дефон\n",
      "Loss: 0.9084204459190368\n",
      ". не всем записывать не понятно в стол. постмодерна и всё ещё в результате от беловолки, конечно по ст\n",
      "Loss: 0.9193012809753418\n",
      ". не открывается не получится, вот только не понятно в смысле может получится, в самом деле уже произв\n",
      "Loss: 0.9084483575820923\n",
      ". не возможность на тебе на стол с конца. в чём угодно по переводить контекст, конечно записывать не п\n",
      "Loss: 0.9134923672676086\n",
      ". не просто транспорт как получится, по рукам да, я просто приговор постмодерну. потому что вы потеря \n",
      "Loss: 0.9073346853256226\n",
      ". становился призовой боец это подряд на пользуюсь свою высокого от передвижения и причин. так это не \n",
      "Loss: 0.901448061466217\n",
      ". не подразумевает верным ну вот это не в собаку, а не вопрос свет с попытка в организме вообще не тол\n",
      "Loss: 0.9094493579864502\n",
      ". не ответ убил может быть на столе. аргументацию, конечно по старой аргументации не поняли в том и пр\n",
      "Loss: 0.8980593085289001\n",
      ". не возможность и подрыв насилить произведением или про последнем круто собственность постановки на п\n",
      "Loss: 0.9125257325172424\n",
      ". не возможность и страны в процессе. не понимаю но вот это постмодернисткий сторона в старый грамотно\n",
      "Loss: 0.899157567024231\n",
      ". дробиться на старом смысла скажем потому что как будто просто тебе на рассказы, хоть не понятно, что\n",
      "Loss: 0.8962817096710205\n",
      ". не вероятность и прочего открыто, но вот они все равно было бы не будет. а вот это может быть концеп\n",
      "Loss: 0.8973429441452027\n",
      ". борьбе может быть про призовые бои как он и просто так и прочего и призовой боец это просто сталь по\n",
      "Loss: 0.9003571438789367\n",
      ". не вот это всё постмодерна и сексуальные слова подождите чем выпал что угодно, кстати, только с пост\n",
      "Loss: 0.894480266571045\n",
      ". не понятно в смысле материальный просто так просто просто и так не получилось а соседи, конечно, чит\n",
      "Loss: 0.9002501463890076\n",
      ". не вот это всё равно не подходит как бы соседи, с полезное оружие? почему так там не подходит как бу\n",
      "Loss: 0.9010324597358703\n",
      ". не возможность и подруга после всего в процессе лишь собственность по вампирски а человек в контекст\n",
      "Loss: 0.8932793140411377\n",
      ". не понятно и прочего отыгрывать написанное игроки на оружие потому что в процессе отсутствие получит\n",
      "Loss: 0.8935089874267578\n",
      ". не хочется на собственность и прочее. при чем тут получил это как бы и всегда про получить и молодых\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8940763521194458\n",
      ". не вероятность и прочее. а если нет минус в произведении. опасное на пользоваться в пустотой и прокл\n",
      "Loss: 0.8889934015274048\n",
      ". не возможность логику это же без сторону сторону про собственной произведениях в подможно из количес\n",
      "Loss: 0.8973194313049316\n",
      ". не ответ доставить откровенно, интересно был про них как то так то как бы, да есть становится в стол\n",
      "Loss: 0.895671215057373\n",
      ". не возможность после всех в пользу своего силой на пользования не подскажите какой нить призовые бои\n",
      "Loss: 0.8945098686218261\n",
      ". не ответил от себя делает персонажа. но не понял, что судьбы заходит смотрел бы не подходит как даже\n",
      "Loss: 0.8952356815338135\n",
      ". не возможно, это не может быть конструкции некоторые можно показания и прочее. имеется ввиду что он \n",
      "Loss: 0.8846961545944214\n",
      ". ахх х это не могут под пользуетесь на себе должен быть собственность по вопрос могут слова потому чт\n",
      "Loss: 0.8874156212806702\n",
      ". не этот проблема в процессе они можно просто с аристократических да и да. но в том, что он не сможет\n",
      "Loss: 0.8881080436706543\n",
      ". да потом они так они не заскрыть что ты не сможет выйти из за стрельбу играть в финт да просто сложн\n",
      "Loss: 0.8937333631515503\n",
      ". стопроц был поэтому в собственной роде произведение? от собрать сильно сильно признаёт домолация, по\n",
      "Loss: 0.892088794708252\n",
      ". не возможность после всего в процентном культуры и потому что про подходит в смысле там в глазами и \n",
      "Loss: 0.8789159059524536\n",
      ". не понимаю но вот бана и всё равно они от гениально про слова стопроц в принципе не подразумевает вс\n",
      "Loss: 0.8861093664169312\n",
      ". не понимаю но нет, это тот серьёзно в процессе. не понял, что они не подобными люди сразу не могу на\n",
      "Loss: 0.881141676902771\n",
      ". борода в контексте не стоит произведение? эх, да и не выступает на столе. информации это просто так \n",
      "Loss: 0.8793135762214661\n",
      ". так это всё ещё в разного другого про самосбор на стол какие конкретики девочка, потому что в себе с\n",
      "Loss: 0.8808449244499207\n",
      ". не возможность и прочего ответственность с большим меняет от приглашён под попытка продолжит цепочку\n",
      "Loss: 0.8759319591522217\n",
      ". не возможность профессиональный для персонажа в отряде сменить в собственной болезни. там только так\n",
      "Loss: 0.8780099868774414\n",
      ". не подходит как провельности про кондитерского изделия? это всё ещё клички можно под помощнение в пр\n",
      "Loss: 0.8828997659683228\n",
      ". потом в смысле мастер с собой с конца. в процессе и собак не подразумевает в смысле по старой под ст\n",
      "Loss: 0.8751595115661621\n",
      ". борять в переворит в контексте совершенно не просто так просто пример предлагает про крыло просто не\n",
      "Loss: 0.8759384441375733\n",
      ". не возможность и тот собаки на призовой бой. под больше строиться но спасибо за приколы на стрельбу \n",
      "Loss: 0.882652587890625\n",
      ". не вопрос мой мастерской и прочие по канату не такой инструмент, как произведения и вообще не показа\n",
      "Loss: 0.875104796886444\n",
      ". борода в первую отношений в подмостировать полноценный выбор из собака сладкого мастерства и попадат\n",
      "Loss: 0.8714273238182068\n",
      ". в пасьязман так то не подождите про подходит про встречу надо получить чтото подразумевает в сторону\n",
      "Loss: 0.880610237121582\n",
      ". не водить дальше на них может стрелял морте в контексте согласился с собой как просто пример пример \n",
      "Loss: 0.8682444429397583\n",
      ". не вообще не понимаю но не просто с добавить кондитерский, смысла как просто так просто так одобряем\n",
      "Loss: 0.8795570468902588\n",
      ". не возможность по канату и не стоит про кондитерского возбуждения применим на выход может быть конкр\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m model(train, hidden)\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m), target\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = RNN(input_size=len(idx_to_char), hidden_size=256, embedding_size=64, n_layers=4)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    patience = 5,\n",
    "    verbose=True,\n",
    "    factor = 0.5\n",
    ")\n",
    "\n",
    "n_epochs = 50000\n",
    "loss_avg = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train, target = get_batch(sequence)\n",
    "    train = train.permute(1, 0, 2).to(device)\n",
    "    target = target.permute(1, 0, 2).to(device)\n",
    "    \n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    output, hidden = model(train, hidden)\n",
    "    \n",
    "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_avg.append(loss.item())\n",
    "    \n",
    "    loss_avg.append(loss.item())\n",
    "    if len(loss_avg) >= 50:\n",
    "        mean_loss = np.mean(loss_avg)\n",
    "        print(f'Loss: {mean_loss}')\n",
    "        scheduler.step(mean_loss)\n",
    "        loss_avg = []\n",
    "        model.eval()\n",
    "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
    "        print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2952c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7969794607162476\n",
      ". не всем сладкий производстве, использовать самый манипуляция по столом. так император и призовой бой\n",
      "Loss: 0.7948158359527588\n",
      ". или это не подобной культуры и прочие ответственность и прочие открывания нет такого контексте подры\n",
      "Loss: 0.7871641254425049\n",
      ". не возможность после величайшим и прочим выбор стража мастер, но сколько перевод в статов с конца, к\n",
      "Loss: 0.7892266988754273\n",
      ". да я сырок, который специалист по старо и прочее. имеется ввиду в отряде оружие? после всего получае\n",
      "Loss: 0.7934277868270874\n",
      ". не вот это просто с контексте реалистичности? потому что там произведение? почерком? а какой том, чт\n",
      "Loss: 0.7880365705490112\n",
      ". потом не подходит, в попытках просто в своём решение не понятно в том, что это истина. ну вот вот та\n",
      "Loss: 0.788667869567871\n",
      ". не возможность после всего в процессе. не понимаю но не выстрел имеет запертый мальчик ты же просто \n",
      "Loss: 0.7872054648399353\n",
      ". не вопрос какие можно подождите представляет игроков на результат как бы, прикол в произведении, про\n",
      "Loss: 0.7905313611030579\n",
      "Epoch 00438: reducing learning rate of group 0 to 1.9531e-05.\n",
      ". не возможно, это в процессе. не открывается но вот это всё ещё в раз с конца. он был после всего это\n",
      "Loss: 0.7884457397460938\n",
      ". не вопрос мне тоже не понял, что они должны подождите картинуть в вас людей, понятно и не подходит к\n",
      "Loss: 0.7901398396492004\n",
      ". не вижу свою игру в контексте соотновечности, потому что всё равно было смотреть, как и не полноценн\n",
      "Loss: 0.7855084037780762\n",
      ". собрать своих ведь обычно делают в любом случае моя?! нпс деда прикольно просто так просто тема в то\n",
      "Loss: 0.7913333225250244\n",
      ". или это не в него играть? в своём результат в процессе и самодечное столько в этом стали по сюжету ч\n",
      "Loss: 0.7964831948280334\n",
      ". не возможность призовой боец это то, который делает постмодерну. так это доставать из собственность \n",
      "Loss: 0.796238718032837\n",
      ". так это просто с собой выше, но не высокой системе, в чём их никто не обосрался, потому что там пров\n",
      "Loss: 0.7847414970397949\n",
      ". не возможно, это не достигаешь доспех, но это же добавить, что вы воздействия и всё ещё в глаза ты и\n",
      "Loss: 0.7890185523033142\n",
      ". а какая силой с конца. если ты её передачи, что он не могут возможность после всего в том а потому ч\n",
      "Loss: 0.7913871908187866\n",
      ". то че просто слова он не обязательно и скажи, что он не дописан. ну вот вот так просто в него подобн\n",
      "Loss: 0.7855128264427185\n",
      ". так там просто не спорт это не всегда понимаю но не получится, возможно, с ним не про подходит конди\n",
      "Loss: 0.790110957622528\n",
      ". не возможность и прочее и прочие открывания неприязнь? это всё равно не получится, возможно, потому \n",
      "Loss: 0.787886962890625\n",
      ". старой для персонажа и не доставая и становится в старый вопрос открыть конкретно бортика а если пос\n",
      "Loss: 0.7906027889251709\n",
      "Epoch 00451: reducing learning rate of group 0 to 9.7656e-06.\n",
      ". а, деньги попадает в одном из суть после совершенства а как бы казайся только споры и прочих любител\n",
      "Loss: 0.7893237686157226\n",
      ". не подразумевает себя после всего в процессе. не подобно от представление формы над собственности! в\n",
      "Loss: 0.793115861415863\n",
      ". потом стреляют дурпудического в раза имеет становится истории не сознательно подразумевает сила и не\n",
      "Loss: 0.7923421025276184\n",
      ". не возможность с разной много себе только что так это не могу интересная такой стороны, потому что в\n",
      "Loss: 0.7882555747032165\n",
      ". не возможность призовой боец это то не понимаю но нет, только в сторону под собой специалист по стар\n",
      "Loss: 0.7877337598800659\n",
      ". не возможно, это не понятно, что он не ответить, что не больше кончались на стрельбу и мастер, так к\n",
      "Loss: 0.7897047257423401\n",
      "Epoch 00457: reducing learning rate of group 0 to 4.8828e-06.\n",
      ". с пол самом деле можно, что по вопросу на подробностей, все карман в том грозовая трогать за сильной\n",
      "Loss: 0.7924819970130921\n",
      ". не подразумевает себя возможность после ваша кубы на полном и дело, что по столом. и просто не всем \n",
      "Loss: 0.7968958568572998\n",
      ". а, деньги по силе. потому что в столярной мастерской и прочитать бы подразумевает в смысле материаль\n",
      "Loss: 0.7946640777587891\n",
      ". потом в том, что вы выше этого в переливается нормально постмодернистких человек в трепет а потом он\n",
      "Loss: 0.7839827847480774\n",
      ". не возможность и его человека кино имеют бары можно под собака у нас постмодернистких дурачков, но в\n",
      "Loss: 0.7841662216186523\n",
      ". не возможность и прочего ответ как бы про них собственность и прочего ответить на ваших глазах слиза\n",
      "Loss: 0.7845528960227967\n",
      ". не вообще не понимаю но не просто с контексте не скровать отношения, так как бы, как можно понять с \n",
      "Loss: 0.7899240732192994\n",
      ". не забавно про них какойто концепта не ответ было бы почему так не подождите просто не вопрос в смыс\n",
      "Loss: 0.7920929789543152\n",
      ". не возможно, это в процессе. не понимаююю я напригильного концепты и общества на становился в процес\n",
      "Loss: 0.7936070895195008\n",
      ". не вот так просто всех оборотней ну это не проблемы и прочим есть персонаж по первый вопрос в процес\n",
      "Loss: 0.7872876977920532\n",
      "Epoch 00467: reducing learning rate of group 0 to 2.4414e-06.\n",
      ". не возможность или в любом случае можно просто с полка проблемы и прочим есть приколов это будет не \n",
      "Loss: 0.7922194409370422\n",
      ". стопроц баланс недоступная? это комплекса и прочего нальтати какой нить пришли про страдания и осозн\n",
      "Loss: 0.7905469417572022\n",
      ". а, друг для прикол получить и прочее. тип, да. так просто пример получится, собака челюги и такое ра\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    train, target = get_batch(sequence)\n",
    "    train = train.permute(1, 0, 2).to(device)\n",
    "    target = target.permute(1, 0, 2).to(device)\n",
    "    \n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    output, hidden = model(train, hidden)\n",
    "    \n",
    "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_avg.append(loss.item())\n",
    "    \n",
    "    loss_avg.append(loss.item())\n",
    "    if len(loss_avg) >= 50:\n",
    "        mean_loss = np.mean(loss_avg)\n",
    "        print(f'Loss: {mean_loss}')\n",
    "        scheduler.step(mean_loss)\n",
    "        loss_avg = []\n",
    "        model.eval()\n",
    "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
    "        print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdfa8714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". а какой условно, можно подходит на производстве, и там можно просто последнем кроме подчёркивания он имеет вопрос в смысле, прос\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(evaluate(\n",
    "    model, \n",
    "    char_to_idx, \n",
    "    idx_to_char, \n",
    "    temp=0.4, \n",
    "    prediction_len=128, \n",
    "    start_text='. '\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63b002c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "torch.save(model.state_dict(), 'syrchello.pt')\n",
    "\n",
    "with open('char_to_idx.pickle', 'wb') as f:\n",
    "    pickle.dump(char_to_idx, f)\n",
    "\n",
    "with open('idx_to_char.pickle', 'wb') as f:\n",
    "    pickle.dump(idx_to_char, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ceef1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('char_to_idx.pickle', 'rb') as f:\n",
    "    char_to_idx = pickle.load(f)\n",
    "    \n",
    "with open('idx_to_char.pickle', 'rb') as f:\n",
    "    idx_to_char = pickle.load(f)\n",
    "    \n",
    "model = RNN(input_size=len(idx_to_char), hidden_size=256, embedding_size=64, n_layers=4)\n",
    "model.load_state_dict(torch.load('syrchello.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7782769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "with Path('syr_text.txt').open() as f:\n",
    "    syr_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94115b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'о', 'а', 'е', 'т', 'и', 'н', 'с', 'р', 'в', 'к', 'л', 'м', 'п', 'у', 'д', 'ь', ',', 'ы', 'б', 'я', 'г', 'ч', 'з', 'х', 'ж', 'й', '.', 'ш', 'э', '?', 'ю', 'ц', 'ё', 'щ', 'ф', '!', 'ъ']\n"
     ]
    }
   ],
   "source": [
    "def text_to_seq(text_sample):\n",
    "    char_count = Counter(text_sample)\n",
    "    char_count = sorted(char_count.items(), key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    sorted_chars = [char for char, _ in char_count]\n",
    "    print(sorted_chars)\n",
    "    char_to_idx = {char: index for index, char in enumerate(sorted_chars)}\n",
    "    idx_to_char = {V: k for k, V in char_to_idx.items()}\n",
    "    sequence = np.array([char_to_idx[char] for char in text_sample])\n",
    "    return sequence, char_to_idx, idx_to_char\n",
    "sequence, _, _ = text_to_seq(syr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26f0d031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4966599893569947\n",
      ". какой просто выставать нужно просто вот то пример и привет стол, на самом стоит в просто в том, что \n",
      "Loss: 1.4383606719970703\n",
      ". не обвания и не просто сталь в моём игры может подходит в процессе столее, но там в первым и прочее \n",
      "Loss: 1.391086254119873\n",
      ". не возможность постмодернисты и получиться с собой смешно и тоже должно быть контексте не надо постм\n",
      "Loss: 1.3588665914535523\n",
      ". получилась как в какой сторону просто после всего а скажу, на старой слова не получится в произведь \n",
      "Loss: 1.3576400756835938\n",
      ". самосбору не подружка вот только в столярность и проблемы в столей стоит в конце не просто с первого\n",
      "Loss: 1.3618614768981934\n",
      ". или в том, что они примитивные подсказки в подобной конечно по странице, почему конечно просто тут с\n",
      "Loss: 1.3535090494155884\n",
      ". не про стоит по старой себя просто словов от тебе не признание как в том и вообще не ответить, пороч\n",
      "Loss: 1.343941502571106\n",
      ". потом собрать в процентном а подобной контекста показательствие как попустит возможность своими над \n",
      "Loss: 1.3473644638061524\n",
      ". а, просто вероятно не просто не подводный взгляд в игра в подобных приколов просто постмодернизме в \n",
      "Loss: 1.3648099613189697\n",
      ". потеря на три собственность и просто признание на пропорное стоит про попку не понимаешь, что они пр\n",
      "Loss: 1.375405616760254\n",
      ". не подружка. это всё на пользования и просто подряд тематика и потому что ты не подругой как в чате \n",
      "Loss: 1.3458500289916993\n",
      ". ахххххах, стреляют на стреляют про собственном собака про найти вопрос по теме от расстроите? а може\n",
      "Loss: 1.3409700107574463\n",
      ". саблю отсутствие продолжа получается на то стоит постановки и постмодерна в смысле кончение получитс\n",
      "Loss: 1.3322683668136597\n",
      ". потому что в своём поставить на полном и про призовой бой потому что в постановки на пользоваться по\n",
      "Loss: 1.3385009574890137\n",
      ". не подобности с пользования не только призовые бои тем более что ты спасибо за прикольное водить в п\n",
      "Loss: 1.3346448469161987\n",
      ". не это не стоит по столом с новый господи, а не собственность и стол с собой не про призовые бои как\n",
      "Loss: 1.3363425397872926\n",
      ". не про них не порфит на разные играть не есть призовой бой призовой боец это не становится в подобны\n",
      "Loss: 1.3275823402404785\n",
      ". не просто не совсем стоит под контексте стоит призовые бои не подробно ли в сторону с метра тоже в с\n",
      "Loss: 1.3358738613128662\n",
      ". не возмущайся на уже на столе? стоит соблазнимс ммм, в смысле действие. на принято было бы потому чт\n",
      "Loss: 1.321781883239746\n",
      ". становил не состояния, не подруги на стол с наличии и прочим самый вопрос можно делающим под в проце\n",
      "Loss: 1.3329974603652954\n",
      ". не при конца не стоит и сеттинг странности и все при чем тут призовые бои не подружками из собака пр\n",
      "Loss: 1.3236007738113402\n",
      ". не всё постмодернисткий кандидат с конца. ну вот сказать не подходит так просто было под поставить к\n",
      "Loss: 1.3338354587554933\n",
      ". подобного против так так в смысле не сломанной столеханием не всегда в смысле, не возможно, в тело н\n",
      "Loss: 1.3205016469955444\n",
      ". не особо не вопрос потому что в стола и не брат ты вообще не спорта и произведения и не получился в \n",
      "Loss: 1.3272383832931518\n",
      ". обсуждаем надо было в сердце или про сеттинг на столом. и вообще по получиться к собой на подводо ст\n",
      "Loss: 1.3396341037750243\n",
      ". какой не получится и про сеттинг не подходит как в конце не проблемы после трипу рассказывает получи\n",
      "Loss: 1.337440357208252\n",
      ". не в сторону с собой, собака почитать и понятно он вообще не по канату от порома, не смог по персона\n",
      "Loss: 1.3279645586013793\n",
      ". не просто и стануть себе на пользоваться в себе не было в становлении труп. определение конечно лицо\n",
      "Loss: 1.3331174993515014\n",
      ". не потеря аргументации не вообще, машина про сказали написанные принято просто с ним про поступки ко\n",
      "Loss: 1.3459618186950684\n",
      "Epoch 00030: reducing learning rate of group 0 to 5.0000e-03.\n",
      ". а не вот это всё ещё в столова но и понимаешь? но это не постановка и всё есть играть в совершенно н\n",
      "Loss: 1.302128052711487\n",
      ". потому что в пользу меня в инструмент и прочим силой произведениях, в то время одновременно социалку\n",
      "Loss: 1.2465215730667114\n",
      ". не подругой контекст в том, что вы возраст и напрягаться в себе должен быть на представленный подтве\n",
      "Loss: 1.2200026893615723\n",
      ". не в своём вперди все на просто не понял, что не вопрос в итоге, получается можно просто они просто \n",
      "Loss: 1.2179588270187378\n",
      ". потому что он получается в том, что вообще как вот просто не стоит под пользовались на прочитай и пр\n",
      "Loss: 1.2043992233276368\n",
      ". а какой нить блока и самый получилось слова не подругой согласился возможность под минус отвечает по\n",
      "Loss: 1.1951818943023682\n",
      ". от просто пример с большим короткий планом с принять детектива культуры с представляется от нас соба\n",
      "Loss: 1.1752689790725708\n",
      ". хорошо на высокое мужики в разные соответственно хороший навык в самом деле на старом смысле на стар\n",
      "Loss: 1.174717240333557\n",
      ". почему нет круто своим замка нет не подруга и прочие столом, который соглашается на произведение в ч\n",
      "Loss: 1.1727498626708985\n",
      ". не подругой карман и получил с ним помогает скажем, которые призовые бои как возможность и так как в\n",
      "Loss: 1.1766387701034546\n",
      ". не просто не сознание на попадает в собственного народа мастер не получилось с кого они просто и не \n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    patience = 5,\n",
    "    verbose=True,\n",
    "    factor = 0.5\n",
    ")\n",
    "\n",
    "loss_avg = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    train, target = get_batch(sequence)\n",
    "    train = train.permute(1, 0, 2).to(device)\n",
    "    target = target.permute(1, 0, 2).to(device)\n",
    "    \n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    output, hidden = model(train, hidden)\n",
    "    \n",
    "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_avg.append(loss.item())\n",
    "    \n",
    "    loss_avg.append(loss.item())\n",
    "    if len(loss_avg) >= 50:\n",
    "        mean_loss = np.mean(loss_avg)\n",
    "        print(f'Loss: {mean_loss}')\n",
    "        scheduler.step(mean_loss)\n",
    "        loss_avg = []\n",
    "        model.eval()\n",
    "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
    "        print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "690b7e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7116520094871521\n",
      ". получается это же делают буривух да в сеттинге средневековой руси будет сказал, что он не понял гово\n",
      "Loss: 0.7156209921836854\n",
      ". в паб и настроение неправда не дописывать не такой не про потеря черновики в найти примитивных конте\n",
      "Loss: 0.7136674666404724\n",
      ". стальа не подруга на производство? и что он не образные круто с собой, он гдето стоит его нельзя нос\n",
      "Loss: 0.7106934332847595\n",
      ". похоже на становится играть в собственной правоте это же не сможет ли вообще всё ещё в результать, д\n",
      "Loss: 0.7184750461578369\n",
      ". не видел в переводить как можно просто не могу в термин норм, лол, нет. это просто термины. как вам \n",
      "Loss: 0.7115459704399109\n",
      "Epoch 00246: reducing learning rate of group 0 to 3.9063e-05.\n",
      ". а, совершенно не заскучать жестоко он больше крови в процессе и тоже по получится, рассказывают отра\n",
      "Loss: 0.7119797396659852\n",
      ". потом и при чём тут потому что он не обязательно хорошо в сердце могут все пример примерами, в стары\n",
      "Loss: 0.7149538803100586\n",
      ". пользуясь как мужчина в конце удар, там подпивасника и всё такое открыл с модерная играть? в сердце \n",
      "Loss: 0.7106159996986389\n",
      ". сказать ответного девочка и прочим мастер на переходе между главное как насчёт его в произведении, т\n",
      "Loss: 0.7148685693740845\n",
      ". соблазними и прочего. мужчина с контексте стоит под сути ты гей был под стол, при чем тут подобной к\n",
      "Loss: 0.7116190361976623\n",
      ". в паб и не мог не подходит как бы такой произведением, я не хочется в безопасности и прочим есть мес\n",
      "Loss: 0.7131230306625366\n",
      "Epoch 00252: reducing learning rate of group 0 to 1.9531e-05.\n",
      ". не видел в стол, вопрос какимито и прочим написать играть в собственной правоте так просто с ними не\n",
      "Loss: 0.7164263916015625\n",
      ". не обоснование деньги и не понял хорошая произведением, я не хочется в своём игру и где умер контекс\n",
      "Loss: 0.7125052881240844\n",
      ". не просто с нейросеть написано но вот опять на территории как просто с ними не буду про популярность\n",
      "Loss: 0.707089524269104\n",
      ". под собака про своих вампиров разве не постановки на него поступается это всё такое разве не добавит\n",
      "Loss: 0.7132077169418335\n",
      ". или по канату не ответишь на первый вампиров у нас в серьёзное у него моркулины и долго. пока у них \n",
      "Loss: 0.7149610328674316\n",
      ". похоже на пользуюсь с конца не даёт джорджи, чуть двойной кайф просто не спортами у него поставить п\n",
      "Loss: 0.7085337400436401\n",
      "Epoch 00258: reducing learning rate of group 0 to 9.7656e-06.\n",
      ". не видел в стойку с признает в переходе с модерна, но не понятно с портрета не вывозить, что не прос\n",
      "Loss: 0.7102717542648316\n",
      ". похоже на тобой то есть те система а вот это всё ещё в любом случае моя?! нпс деда движением руки и \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m hidden \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_hidden(BATCH_SIZE)\n\u001b[0;32m----> 9\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m), target\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, hidden):\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     out, (ht1, ct1) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    773\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    train, target = get_batch(sequence)\n",
    "    train = train.permute(1, 0, 2).to(device)\n",
    "    target = target.permute(1, 0, 2).to(device)\n",
    "    \n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    output, hidden = model(train, hidden)\n",
    "    \n",
    "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_avg.append(loss.item())\n",
    "    \n",
    "    loss_avg.append(loss.item())\n",
    "    if len(loss_avg) >= 50:\n",
    "        mean_loss = np.mean(loss_avg)\n",
    "        print(f'Loss: {mean_loss}')\n",
    "        scheduler.step(mean_loss)\n",
    "        loss_avg = []\n",
    "        model.eval()\n",
    "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
    "        print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f970d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
